{"cells":[{"metadata":{"id":"900A996DF3114CBE8545153F973B4640","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# softmax和分类模型\n## softmax\n- 置信度：输出 $\\underset{i}{\\arg\\max} o_i$。\n- 小批量矢量计算表达式：广播机制 \n\n## 交叉熵损失函数\n对于训练数据集的样本数为$n$，交叉熵损失函数定义为 \n$$\n\\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right )=\\frac{1}{n} \\sum_{i=1}^n (-\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)}),\n$$"},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","graffitiCellId":"id_my8ejol","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9C804F2DABDA482BB4375CB0D36E011D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x864 with 10 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9C804F2DABDA482BB4375CB0D36E011D/q5osfjg4l4.svg\">"},"transient":{}}],"source":"#加载读取数据集\n%matplotlib inline\nfrom IPython import display\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport time\n\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nmnist_train = torchvision.datasets.FashionMNIST(root='/home/kesci/input/FashionMNIST2065', train=True, download=True, transform=transforms.ToTensor())\nmnist_test = torchvision.datasets.FashionMNIST(root='/home/kesci/input/FashionMNIST2065', train=False, download=True, transform=transforms.ToTensor())\n\nmnist_PIL = torchvision.datasets.FashionMNIST(root='/home/kesci/input/FashionMNIST2065', train=True, download=True)\n\ndef get_fashion_mnist_labels(labels):\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n    \ndef show_fashion_mnist(images, labels):\n    d2l.use_svg_display()\n    # 这里的_表示我们忽略（不使用）的变量\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.view((28, 28)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)\n    plt.show()\n    \nX, y = [], []\nfor i in range(10):\n    X.append(mnist_train[i][0]) # 将第i个feature加到X中\n    y.append(mnist_train[i][1]) # 将第i个label加到y中\nshow_fashion_mnist(X, get_fashion_mnist_labels(y))\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_0nzlmif","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7886893A27B44B979B7E4B93F870B3CE","mdEditEnable":false},"source":"# softmax从零开始的实现"},{"cell_type":"code","execution_count":6,"metadata":{"graffitiCellId":"id_c0aor6l","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C47E4519B73C46558F23670B6C6D2CF4","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.7840, train acc 0.750, test acc 0.789\nepoch 2, loss 0.5703, train acc 0.813, test acc 0.811\nepoch 3, loss 0.5244, train acc 0.826, test acc 0.818\nepoch 4, loss 0.5021, train acc 0.832, test acc 0.825\nepoch 5, loss 0.4856, train acc 0.837, test acc 0.826\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x864 with 9 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/C47E4519B73C46558F23670B6C6D2CF4/q5ossf6gmt.svg\">"},"transient":{}}],"source":"import torch\nimport torchvision\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\n#加载数据集\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')\n\n#模型参数初始化\nnum_inputs = 784\nnum_outputs = 10\n\nW = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)\nb = torch.zeros(num_outputs, dtype=torch.float)\n\nW.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)\n\n#定义softmax\ndef softmax(X):\n    X_exp = X.exp()\n    partition = X_exp.sum(dim=1, keepdim=True)\n    return X_exp / partition  # 这里应用了广播机制\n    \ndef net(X):\n    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)\n    \n#定义损失函数\ndef cross_entropy(y_hat, y):\n    return - torch.log(y_hat.gather(1, y.view(-1, 1)))\n\n#定义准确率\ndef accuracy(y_hat, y):\n    return (y_hat.argmax(dim=1) == y).float().mean().item()\n\ndef evaluate_accuracy(data_iter, net):\n    acc_sum, n = 0.0, 0\n    for X, y in data_iter:\n        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n        n += y.shape[0]\n    return acc_sum / n\n    \n#训练模型\nnum_epochs, lr = 5, 0.1\n\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n              params=None, lr=None, optimizer=None):\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n        for X, y in train_iter:\n            y_hat = net(X)\n            l = loss(y_hat, y).sum()\n            \n            # 梯度清零\n            if optimizer is not None:\n                optimizer.zero_grad()\n            elif params is not None and params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            \n            l.backward()\n            if optimizer is None:\n                d2l.sgd(params, lr, batch_size)\n            else:\n                optimizer.step() \n            \n            \n            train_l_sum += l.item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n            n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\ntrain_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)\n\n#模型预测\nX, y = iter(test_iter).next()\n\ntrue_labels = d2l.get_fashion_mnist_labels(y.numpy())\npred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())\ntitles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n\nd2l.show_fashion_mnist(X[0:9], titles[0:9])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_623cj7f","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"740E43F6246C47078E68E5EB4399345A","mdEditEnable":false},"source":"# softmax的简洁实现"},{"cell_type":"code","execution_count":8,"metadata":{"graffitiCellId":"id_s0m2c8a","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"94B0411C93ED4E6C83B5545D9339E395","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.0031, train acc 0.750, test acc 0.780\nepoch 2, loss 0.0022, train acc 0.813, test acc 0.806\nepoch 3, loss 0.0021, train acc 0.827, test acc 0.821\nepoch 4, loss 0.0020, train acc 0.832, test acc 0.817\nepoch 5, loss 0.0019, train acc 0.836, test acc 0.824\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x864 with 9 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/94B0411C93ED4E6C83B5545D9339E395/q5ot0bqdry.svg\">"},"transient":{}}],"source":"# 加载各种包或者模块\nimport torch\nfrom torch import nn\nfrom torch.nn import init\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\n#加载数据及初始化\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')\n\nnum_inputs = 784\nnum_outputs = 10\n\nclass LinearNet(nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super(LinearNet, self).__init__()\n        self.linear = nn.Linear(num_inputs, num_outputs)\n    def forward(self, x): # x 的形状: (batch, 1, 28, 28)\n        y = self.linear(x.view(x.shape[0], -1))\n        return y\n    \nclass FlattenLayer(nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x): \n        return x.view(x.shape[0], -1)\n\nfrom collections import OrderedDict\nnet = nn.Sequential(\n       \n        OrderedDict([\n           ('flatten', FlattenLayer()),\n           # 等价于 LinearNet(num_inputs, num_outputs)\n           ('linear', nn.Linear(num_inputs, num_outputs))])\n        )\n        \n#初始化模型参数\ninit.normal_(net.linear.weight, mean=0, std=0.01)\ninit.constant_(net.linear.bias, val=0)\n\n#定义损失函数\nloss = nn.CrossEntropyLoss() \n# class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n\n#定义优化函数\noptimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n# class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n\n#训练\nnum_epochs = 5\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)\n\n#模型预测\nX, y = iter(test_iter).next()\n\ntrue_labels = d2l.get_fashion_mnist_labels(y.numpy())\npred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())\ntitles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n\nd2l.show_fashion_mnist(X[0:9], titles[0:9])"},{"metadata":{"id":"468F003C912645D0884645850EFF2402","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}