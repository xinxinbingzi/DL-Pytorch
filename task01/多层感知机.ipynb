{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_46frs9w","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D89AFB24BD6B48F698E0F6DEFD4F4051","mdEditEnable":false},"source":"# 多层感知机\n## 激活函数\n全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。\n\n### 关于激活函数的选择\n- ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。\n- 用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  \n- 在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。\n- 在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。\n\n### 多层感知机\n多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：\n$$\n \\begin{aligned} \\boldsymbol{H} &= \\phi(\\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h),\\\\ \\boldsymbol{O} &= \\boldsymbol{H} \\boldsymbol{W}_o + \\boldsymbol{b}_o, \\end{aligned} \n$$\n其中$\\phi$表示激活函数。"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ie66nbg","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FFFB468B5E804BBE96DA88180CAF5E7A","mdEditEnable":false},"source":"## 多层感知机从零开始的实现"},{"cell_type":"code","execution_count":1,"metadata":{"graffitiCellId":"id_6a7tvf7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"78607977A6E44F26AE3FCB420BFB8692","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.0030, train acc 0.714, test acc 0.722\nepoch 2, loss 0.0019, train acc 0.823, test acc 0.784\nepoch 3, loss 0.0017, train acc 0.845, test acc 0.822\nepoch 4, loss 0.0016, train acc 0.854, test acc 0.813\nepoch 5, loss 0.0015, train acc 0.863, test acc 0.848\n","name":"stdout"}],"source":"import torch\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\n#加载训练集\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root='/home/kesci/input/FashionMNIST2065')\n\n#定义模型参数\nnum_inputs, num_outputs, num_hiddens = 784, 10, 256\n\nW1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)\nb1 = torch.zeros(num_hiddens, dtype=torch.float)\nW2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\nb2 = torch.zeros(num_outputs, dtype=torch.float)\n\nparams = [W1, b1, W2, b2]\nfor param in params:\n    param.requires_grad_(requires_grad=True)\n    \n#定义激活函数\ndef relu(X):\n    return torch.max(input=X, other=torch.tensor(0.0))\n    \n#定义网络\ndef net(X):\n    X = X.view((-1, num_inputs))\n    H = relu(torch.matmul(X, W1) + b1)\n    return torch.matmul(H, W2) + b2\n    \n#定义损失函数\nloss = torch.nn.CrossEntropyLoss()\n\n#训练\nnum_epochs, lr = 5, 100.0\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_vlyl5rw","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BA81101ED80E4F12982C8145D848F8F3","mdEditEnable":false},"source":"## 多层感知机pytorch实现"},{"cell_type":"code","execution_count":2,"metadata":{"graffitiCellId":"id_y3ww285","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"717B8BFCFA264BF18F5A0A4972F73194","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.0031, train acc 0.705, test acc 0.758\nepoch 2, loss 0.0019, train acc 0.822, test acc 0.765\nepoch 3, loss 0.0017, train acc 0.843, test acc 0.832\nepoch 4, loss 0.0015, train acc 0.855, test acc 0.798\nepoch 5, loss 0.0014, train acc 0.864, test acc 0.856\n","name":"stdout"}],"source":"import torch\nfrom torch import nn\nfrom torch.nn import init\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\n#模型初始化\nnum_inputs, num_outputs, num_hiddens = 784, 10, 256\n    \nnet = nn.Sequential(\n        d2l.FlattenLayer(),\n        nn.Linear(num_inputs, num_hiddens),\n        nn.ReLU(),\n        nn.Linear(num_hiddens, num_outputs), \n        )\n    \nfor params in net.parameters():\n    init.normal_(params, mean=0, std=0.01)\n\n#训练    \nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root='/home/kesci/input/FashionMNIST2065')\nloss = torch.nn.CrossEntropyLoss()\n\noptimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n\nnum_epochs = 5\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}