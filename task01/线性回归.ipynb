{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3xxr5fb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B116BFDF0D464FF49A85A582357D0B4D","mdEditEnable":false},"source":"# 线性回归\n## 两个关键点\n### 损失函数\n常选取一个非负数作为误差，比如平方函数，在线性系统中评估索引为 $i$ 的样本误差的表达式为\n$$\nl^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,\n$$\n小批量样本$n$的平均样本误差为\n$$\nL(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.\n$$\n### 优化函数 - 随机梯度下降\n小批量随机梯度下降（mini-batch stochastic gradient descent）：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   \n$$\n(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)\n$$  \n学习率: $\\eta$代表在每次优化中，能够学习的步长的大小    \n批量大小: $\\mathcal{B}$是小批量计算中的批量大小batch size   \n\n优化步骤：\n- (i)初始化模型参数，一般来说使用随机初始化；\n- (ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3y8h3t7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"84D91561397548D7ACB5FAB71E66AB9B","mdEditEnable":false},"source":"## 线性回归模型从零开始的实现\n\n"},{"cell_type":"code","execution_count":4,"metadata":{"graffitiCellId":"id_3snj2zc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B3148881D9514B898929430997FD781C","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.038264\nepoch 2, loss 0.000150\nepoch 3, loss 0.000051\nepoch 4, loss 0.000050\nepoch 5, loss 0.000050\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(tensor([[ 1.9998],\n         [-3.3998]], requires_grad=True),\n [2, -3.4],\n tensor([4.2003], requires_grad=True),\n 4.2)"},"transient":{},"execution_count":4}],"source":"# 引入模块\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\n\n# 生成数据集\n# 设置输入特征数 \nnum_inputs = 2\n# 设置样本数\nnum_examples = 1000\n\n# 设置真实权重和偏差值，以取得真实标签\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nfeatures = torch.randn(num_examples, num_inputs,\n                      dtype=torch.float32)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n                       dtype=torch.float32)\n\n#显示样本分布\n#plt.scatter(features[:, 1].numpy(), labels.numpy(), 1);\n\n#读取数据集\ndef data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    # 随机打乱样本\n    random.shuffle(indices)  \n    for i in range(0, num_examples, batch_size):\n        # 考虑最后剩余数是否满足\n        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) \n        yield  features.index_select(0, j), labels.index_select(0, j)\n\nbatch_size = 10\n\n#初始化模型参数\nw = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)\nb = torch.zeros(1, dtype=torch.float32)\n#引入参数的梯度值\nw.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)\n\n#定义模型\ndef linreg(X, w, b):\n    return torch.mm(X, w) + b\n\n#定义损失函数\ndef squared_loss(y_hat, y): \n    return (y_hat - y.view(y_hat.size())) ** 2 / 2\n    \n#定义优化函数\ndef sgd(params, lr, batch_size): \n    for param in params:\n        param.data -= lr * param.grad / batch_size # 使用.data来更改参数，不改变梯度\n        \n#训练模型\n# 学习率及迭代次数初始化\nlr = 0.03\nnum_epochs = 5\n\nnet = linreg\nloss = squared_loss\n\n# 训练\nfor epoch in range(num_epochs):  \n    \n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y).sum()  \n        # 计算批量样本损失的梯度\n        l.backward()  \n        # 使用小批量随机梯度下降迭代模型参数\n        sgd([w, b], lr, batch_size)  \n        # 初始化梯度\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n    train_l = loss(net(features, w, b), labels)\n    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))\n    \nw, true_w, b, true_b"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_pi6pxp6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7E8D79B69557446883330AB1E8DE07E2","mdEditEnable":false},"source":"## 线性回归模型使用pytorch的简洁实现\n"},{"cell_type":"code","execution_count":5,"metadata":{"graffitiCellId":"id_sdic11w","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D5CCF3AE67794558930978F1815C38B9","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"LinearNet(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\nSequential(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\nLinear(in_features=2, out_features=1, bias=True)\nParameter containing:\ntensor([[-0.0152,  0.0038]], requires_grad=True)\nParameter containing:\ntensor([0.], requires_grad=True)\nSGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.03\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\nepoch 1, loss: 0.000248\nepoch 2, loss: 0.000124\nepoch 3, loss: 0.000138\n[2, -3.4] tensor([[ 2.0009, -3.3998]])\n4.2 tensor([4.2005])\n","name":"stdout"}],"source":"import torch\nfrom torch import nn\nimport numpy as np\ntorch.manual_seed(1) \n\ntorch.set_default_tensor_type('torch.FloatTensor')  #修改默认的tensor类型\n\n#生成数据集\nnum_inputs = 2\nnum_examples = 1000\n\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nfeatures = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n\n#读取数据集\nimport torch.utils.data as Data\n\nbatch_size = 10\n\ndataset = Data.TensorDataset(features, labels)\n\ndata_iter = Data.DataLoader(\n    dataset=dataset,\n    batch_size=batch_size,  \n    shuffle=True,              \n    num_workers=2,  \n)\n\n#定义模型\nclass LinearNet(nn.Module):\n    def __init__(self, n_feature):\n        super(LinearNet, self).__init__()   \n        # 函数原型: `torch.nn.Linear(in_features, out_features, bias=True)`\n        self.linear = nn.Linear(n_feature, 1) \n\n    def forward(self, x):\n        y = self.linear(x)\n        return y\n    \nnet = LinearNet(num_inputs)\nprint(net)\n\nnet = nn.Sequential(\n    nn.Linear(num_inputs, 1)\n    )\n\nnet = nn.Sequential()\nnet.add_module('linear', nn.Linear(num_inputs, 1))\n\nfrom collections import OrderedDict\nnet = nn.Sequential(OrderedDict([\n          ('linear', nn.Linear(num_inputs, 1))\n        ]))\n\nprint(net)\nprint(net[0])\n\n#初始化模型参数\nfrom torch.nn import init\n\ninit.normal_(net[0].weight, mean=0.0, std=0.01)\ninit.constant_(net[0].bias, val=0.0)  # 等价于`net[0].bias.data.fill_(0)`\n\nfor param in net.parameters():\n    print(param)\n    \n#定义损失函数\nloss = nn.MSELoss()    \n# function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`\n\n#定义优化函数\nimport torch.optim as optim\n# 函数原型: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\noptimizer = optim.SGD(net.parameters(), lr=0.03)  \nprint(optimizer) \n\n#训练\nnum_epochs = 3\nfor epoch in range(1, num_epochs + 1):\n    for X, y in data_iter:\n        output = net(X)\n        l = loss(output, y.view(-1, 1))\n        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()\n        l.backward()\n        optimizer.step()\n    print('epoch %d, loss: %f' % (epoch, l.item()))\n    \ndense = net[0]\nprint(true_w, dense.weight.data)\nprint(true_b, dense.bias.data)"},{"metadata":{"id":"E03A403FD4054B65ACAD0130FECFDA3F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}