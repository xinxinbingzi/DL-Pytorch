{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3xxr5fb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B116BFDF0D464FF49A85A582357D0B4D","mdEditEnable":false},"source":"# 线性回归\n## 关键点\n### 损失函数\n常选取一个非负数作为误差，比如平方函数，在线性系统中评估索引为 $i$ 的样本误差的表达式为\n$$\nl^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,\n$$\n小批量样本$n$的平均样本误差为\n$$\nL(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.\n$$\n### 优化函数 - 随机梯度下降\n解析解（analytical solution）：当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来；\n数值解（numerical solution）：大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。\n\n小批量随机梯度下降（mini-batch stochastic gradient descent）：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   \n$$\n(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)\n$$  \n学习率: $\\eta$代表在每次优化中，能够学习的步长的大小    \n批量大小: $\\mathcal{B}$是小批量计算中的批量大小batch size   \n\n优化步骤：\n- (i)初始化模型参数，一般来说使用随机初始化；\n- (ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3y8h3t7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"84D91561397548D7ACB5FAB71E66AB9B","mdEditEnable":false},"source":"## 线性回归模型从零开始的实现\n\n"},{"cell_type":"code","execution_count":5,"metadata":{"graffitiCellId":"id_3snj2zc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B3148881D9514B898929430997FD781C","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"# import packages and modules\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\n\n#print(torch.__version__)\n\n#...........生成数据集...........#\n# set input feature number \nnum_inputs = 2\n# set example number\nnum_examples = 1000\n\n# set true weight and bias in order to generate corresponded label\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nfeatures = torch.randn(num_examples, num_inputs,\n                      dtype=torch.float32)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n                       dtype=torch.float32)\n\n#display dataset\n#plt.scatter(features[:, 1].numpy(), labels.numpy(), 1);\n\n#...........读取数据集...........#\ndef data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)  # random read 10 samples\n    for i in range(0, num_examples, batch_size):\n        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch\n        yield  features.index_select(0, j), labels.index_select(0, j)\n\nbatch_size = 10\n\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break\n\n#...........初始化模型参数...........#\nw = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)\nb = torch.zeros(1, dtype=torch.float32)\n#引入参数的梯度值\nw.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)\n\n#定义模型\ndef linreg(X, w, b):\n    return torch.mm(X, w) + b\n\n#定义损失函数\ndef squared_loss(y_hat, y): \n    return (y_hat - y.view(y_hat.size())) ** 2 / 2\n    \n#定义优化函数\ndef sgd(params, lr, batch_size): \n    for param in params:\n        param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track\n        \n#训练模型\n# super parameters init\nlr = 0.03\nnum_epochs = 5\n\nnet = linreg\nloss = squared_loss\n\n# training\nfor epoch in range(num_epochs):  # training repeats num_epochs times\n    # in each epoch, all the samples in dataset will be used once\n    \n    # X is the feature and y is the label of a batch sample\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y).sum()  \n        # calculate the gradient of batch sample loss \n        l.backward()  \n        # using small batch random gradient descent to iter model parameters\n        sgd([w, b], lr, batch_size)  \n        # reset parameter gradient\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n    train_l = loss(net(features, w, b), labels)\n    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))\n    \n#w, true_w, b, true_b\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_pi6pxp6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7E8D79B69557446883330AB1E8DE07E2","mdEditEnable":false},"source":"## 线性回归模型使用pytorch的简洁实现\n"},{"cell_type":"code","execution_count":23,"metadata":{"graffitiCellId":"id_sdic11w","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D5CCF3AE67794558930978F1815C38B9","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"import torch\nfrom torch import nn\nimport numpy as np\ntorch.manual_seed(1)  #为CPU设置种子用于生成随机数，以使得结果是确定的\n\n#print(torch.__version__)\ntorch.set_default_tensor_type('torch.FloatTensor')  #修改默认的tensor类型\n\n#生成数据集\nnum_inputs = 2\nnum_examples = 1000\n\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nfeatures = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n\n#读取数据集\nimport torch.utils.data as Data\n\nbatch_size = 10\n\n# combine featues and labels of dataset\ndataset = Data.TensorDataset(features, labels)\n\n# put dataset into DataLoader\ndata_iter = Data.DataLoader(\n    dataset=dataset,            # torch TensorDataset format\n    batch_size=batch_size,      # mini batch size\n    shuffle=True,               # whether shuffle the data or not\n    num_workers=2,              # read data in multithreading\n)\n\nfor X, y in data_iter:\n    print(X, '\\n', y)\n    break\n\n#定义模型\nclass LinearNet(nn.Module):\n    def __init__(self, n_feature):\n        super(LinearNet, self).__init__()      # call father function to init \n        self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`\n\n    def forward(self, x):\n        y = self.linear(x)\n        return y\n    \nnet = LinearNet(num_inputs)\nprint(net)\n\n# ways to init a multilayer network\n# method one\nnet = nn.Sequential(\n    nn.Linear(num_inputs, 1)\n    # other layers can be added here\n    )\n\n# method two\nnet = nn.Sequential()\nnet.add_module('linear', nn.Linear(num_inputs, 1))\n# net.add_module ......\n\n# method three\nfrom collections import OrderedDict\nnet = nn.Sequential(OrderedDict([\n          ('linear', nn.Linear(num_inputs, 1))\n          # ......\n        ]))\n\nprint(net)\nprint(net[0])\n\n#初始化模型参数\nfrom torch.nn import init\n\ninit.normal_(net[0].weight, mean=0.0, std=0.01)\ninit.constant_(net[0].bias, val=0.0)  # or you can use `net[0].bias.data.fill_(0)` to modify it directly\n\nfor param in net.parameters():\n    print(param)\n    \n#定义损失函数\nloss = nn.MSELoss()    # nn built-in squared loss function\n                       # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`\n\n#定义优化函数\nimport torch.optim as optim\n\noptimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent function\nprint(optimizer)  # function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\n\n#训练\nnum_epochs = 3\nfor epoch in range(1, num_epochs + 1):\n    for X, y in data_iter:\n        output = net(X)\n        l = loss(output, y.view(-1, 1))\n        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()\n        l.backward()\n        optimizer.step()\n    print('epoch %d, loss: %f' % (epoch, l.item()))\n    \n# result comparision\ndense = net[0]\nprint(true_w, dense.weight.data)\nprint(true_b, dense.bias.data)"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}